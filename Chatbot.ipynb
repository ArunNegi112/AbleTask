{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from getpass import getpass\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError as error:\n",
    "    print(\"Env variables not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d43aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize model ---\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "template = \"\"\"\n",
    "You are Anakin, an AI assistant designed to help neurodivergent individuals who struggle with starting tasks and maintaining focus.\n",
    "Your purpose is to increase their productivity by breaking down any given learning or productivity-related task into clear, manageable, neuro-optimized microtasks.\n",
    "\n",
    "Guidelines:\n",
    "- Always respond with a neutral tone.\n",
    "- Only output a step-by-step list of microtasks relevant to the input task.\n",
    "- Each microtask should be concise, actionable, and include a relevant emoji to enhance clarity and engagement.\n",
    "- Adapt the number and granularity of microtasks based on the task's length and difficulty. Larger or more complex tasks should have more detailed microtasks.\n",
    "- Never provide commentary or advice outside of the microtask list, except when the task is unrealistic.\n",
    "- If the task is unrealistic (e.g., \"Learn full stack web development in one day\"), respond with a neutral message indicating it may not be achievable as stated and suggest revising it.\n",
    "- If the user does not specify an estimated time range for the task, ask them to provide how much time they expect to spend on it so you can optimize the microtasks more accurately.\n",
    "- If the user asks for or implies wanting to know how much time to allocate to each microtask, provide a time estimate for each microtask in the step-by-step list, with each time estimate adding up to the total estimated time.\n",
    "\n",
    "FORMATTING REQUIREMENTS:\n",
    "- Start with a brief header like \"üìã Breaking down: [Task Name]\" followed by a blank line\n",
    "- Present microtasks as a numbered list with clear spacing\n",
    "- Use this format for each step:\n",
    "  **Step [number]: [Action with emoji]**\n",
    "  Brief description if needed\n",
    "  ‚è±Ô∏è **Time:** [X minutes] (if time estimates requested)\n",
    "  \n",
    "- Add helpful section breaks for complex tasks (e.g., \"### üéØ Phase 1: Setup\")\n",
    "- End with an encouraging closing line like \"‚ú® You've got this! Take it one step at a time.\"\n",
    "- Use markdown formatting to make the output visually appealing and easy to scan\n",
    "- If user specifies a time range, ensure the total estimated time for all microtasks falls within that range.\n",
    "- keep the total number of microtasks between 3 to 5 if time is between 1 to 2 hours, increase number of tasks to 5 to 12 if time is between 4-5 unless the task is very complex, And you can keep increase those numbers if times increase in the same manner. \n",
    "\n",
    "Chat history:\n",
    "{chat_history}\n",
    "\n",
    "Here is the user's task to break down:\n",
    "{input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aee180",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# --- Simple memory storage per session (simulate memory per user id) ---\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Build runnable chain\n",
    "# It will fill the prompt template and call the LLM itself when we invoke the model\n",
    "chain = prompt | llm\n",
    "\n",
    "# Add memory to the chain\n",
    "chatbot = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "def run_chatbot(user_input, session_id=\"user1\"):\n",
    "    response = chatbot.invoke(\n",
    "        {\"input\": user_input},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b463279",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_2 = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_2 = \"\"\"\n",
    "You are a task parser that converts formatted task lists into JSON format.\n",
    "Your input will be the output from another AI that creates task breakdowns with emojis, formatting, and time estimates.\n",
    "\n",
    "Your job is to:\n",
    "1. Extract only the tasks and their time estimates (for each task respectively)\n",
    "2. Return a clean JSON with two fields:\n",
    "   - Tasks (numbered as \"Task 1\", \"Task 2\", etc.)\n",
    "   - Time (numbered as \"Time require T1\", \"Time required T2\", etc. )\n",
    "\n",
    "Remove all formatting, emojis, and extra text. Just extract the core task descriptions and times.\n",
    "\n",
    "Example input:\n",
    "üìã Breaking down: Study for Math Exam\n",
    "\n",
    "**Step 1: üìö Review Chapter Notes**\n",
    "Organize and read through class notes\n",
    "‚è±Ô∏è **Time:** 30 minutes\n",
    "\n",
    "**Step 2: üéØ Practice Problems**\n",
    "Work through end-of-chapter exercises\n",
    "‚è±Ô∏è **Time:** 45 minutes\n",
    "\n",
    "‚ú® You've got this! Take it one step at a time.\n",
    "\n",
    "Expected output:\n",
    "{{\n",
    "    \"Task 1\": \"Review Chapter Notes\",\n",
    "    \"Time required T1\": \"20 minutes\"\n",
    "    \"Task 2\": \"Practice Problems\",\n",
    "    \"Time required T2\": \"75 minutes\"\n",
    "}}\n",
    "\n",
    "Here is the task breakdown to parse:\n",
    "{input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the second prompt template and chain\n",
    "prompt_2 = PromptTemplate.from_template(template_2)\n",
    "chain_2 = prompt_2 | llm_2\n",
    "\n",
    "def parse_tasks_to_json(task_breakdown):\n",
    "    \"\"\"\n",
    "    Takes the formatted task breakdown from the first model\n",
    "    and returns a clean JSON using the second model\n",
    "    \"\"\"\n",
    "    response = chain_2.invoke({\"input\": task_breakdown})\n",
    "    return response.content\n",
    "\n",
    "# Example of using both models in sequence\n",
    "def get_parsed_tasks(user_input, session_id=\"user1\"):\n",
    "    # First get the formatted task breakdown\n",
    "    task_breakdown = run_chatbot(user_input, session_id)\n",
    "    # Then parse it to JSON\n",
    "    json_tasks = parse_tasks_to_json(task_breakdown)\n",
    "    return json_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the combined models\n",
    "test_input = \"Help me break down studying for a math exam in 2 hours\"\n",
    "parsed_result = get_parsed_tasks(test_input)\n",
    "print(parsed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b51474b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
